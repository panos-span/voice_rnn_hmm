{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import re\n",
    "\n",
    "'''\n",
    "Φτιάξτε μία συνάρτηση (data parser) που να διαβάζει όλα τα αρχεία ήχου που δίνονται μέσα στο φάκελο digits/\n",
    "και να επιστρέφει 3 λίστες Python, που να περιέχουν: Το wav που διαβάστηκε με librosa, τον αντίστοιχο ομιλητή\n",
    "και το ψηφίο\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def data_parser(directory):\n",
    "    wavs = []\n",
    "    speakers = []\n",
    "    digits = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        wav , sr = librosa.load(filepath, sr=16000)\n",
    "        wavs.append(wav)\n",
    "        \n",
    "        # Εξαγωγή ψηφίου και ομιλητή από το όνομα του αρχείου\n",
    "        name_part = filename.split('.')[0]\n",
    "        match = re.match(r\"([a-zA-Z]+)(\\d+)\", name_part)\n",
    "        if match:\n",
    "            digits.append(match.group(1))\n",
    "            speakers.append(int(match.group(2)))\n",
    "        else:\n",
    "            digits.append(None)\n",
    "            speakers.append(None)\n",
    "            \n",
    "    return wavs, speakers, digits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Εξάγετε με το librosa τα Mel-Frequency Cepstral Coefficients (MFCCs) για κάθε αρχείο ήχου. Εξάγετε 13\n",
    "χαρακτηριστικά ανά αρχείο. Χρησιμοποιήστε μήκος παραθύρου 25 ms και βήμα 10 ms. Επίσης, υπολογίστε και\n",
    "την πρώτη και δεύτερη τοπική παράγωγο των χαρακτηριστικών, τις λεγόμενες deltas και delta-deltas (hint:\n",
    "υπάρχει έτοιμη υλοποίηση στο librosa).\n",
    "'''\n",
    "\n",
    "import librosa\n",
    "\n",
    "def extract_mfccs(wavs):\n",
    "    mfccs = []\n",
    "    \n",
    "    for wav in wavs:\n",
    "        mfcc = librosa.feature.mfcc(y=wav, sr=16000, n_mfcc=13, n_fft=400, hop_length=160)\n",
    "        delta = librosa.feature.delta(mfcc)\n",
    "        delta2 = librosa.feature.delta(mfcc, order=2)\n",
    "        \n",
    "        mfccs.append((mfcc, delta, delta2))\n",
    "        \n",
    "    return mfccs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Αναπαραστήστε τα ιστογράμματα του 1ου και του 2ου MFCC των ψηφίων n1 (7) και n2 (4) για όλες τους τις\n",
    "εκφωνήσεις. Πόση απόκλιση υπάρχει?\n",
    "'''\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "#from step2 import data_parser\n",
    "#from step3 import extract_mfccs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 2: Load data\n",
    "wavs, speakers, digit_labels = data_parser('./data/digits/')\n",
    "\n",
    "# Step 3: Extract MFCCs\n",
    "mfccs_list = extract_mfccs(wavs)\n",
    "\n",
    "# Digits to analyze\n",
    "digits_to_analyze = ['seven', 'four']\n",
    "\n",
    "# Collect MFCC coefficients for each digit\n",
    "digit_mfccs = {digit: {'mfcc1': [], 'mfcc2': []} for digit in digits_to_analyze}\n",
    "\n",
    "for idx, digit in enumerate(digit_labels):\n",
    "    if digit in digits_to_analyze:\n",
    "        mfcc, delta, delta2 = mfccs_list[idx]\n",
    "        # Collect the 1st and 2nd MFCC coefficients across all frames\n",
    "        digit_mfccs[digit]['mfcc1'].extend(mfcc[0].flatten())\n",
    "        digit_mfccs[digit]['mfcc2'].extend(mfcc[1].flatten())\n",
    "\n",
    "# Plot histograms\n",
    "for digit in digits_to_analyze:\n",
    "    mfcc1_values = np.array(digit_mfccs[digit]['mfcc1'])\n",
    "    mfcc2_values = np.array(digit_mfccs[digit]['mfcc2'])\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Histogram for MFCC 1\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(mfcc1_values, bins=50, color='blue', alpha=0.7)\n",
    "    plt.title(f'Histogram of MFCC 1 for digit \"{digit}\"')\n",
    "    plt.xlabel('MFCC 1 Coefficient Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Histogram for MFCC 2\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(mfcc2_values, bins=50, color='green', alpha=0.7)\n",
    "    plt.title(f'Histogram of MFCC 2 for digit \"{digit}\"')\n",
    "    plt.xlabel('MFCC 2 Coefficient Value')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Save the histograms\n",
    "    plt.savefig(f'images/histograms_{digit}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Calculate and print variance\n",
    "    variance_mfcc1 = np.var(mfcc1_values)\n",
    "    variance_mfcc2 = np.var(mfcc2_values)\n",
    "    print(f'Digit \"{digit}\":')\n",
    "    print(f'Variance of MFCC 1: {variance_mfcc1:.2f}')\n",
    "    print(f'Variance of MFCC 2: {variance_mfcc2:.2f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a mapping from (digit, speaker) to index\n",
    "utterances = {}\n",
    "\n",
    "for idx, (digit, speaker) in enumerate(zip(digit_labels, speakers)):\n",
    "    key = (digit, speaker)\n",
    "    if key not in utterances:\n",
    "        utterances[key] = idx\n",
    "\n",
    "# Select indices for 'seven' and 'four' spoken by speakers 1 and 2\n",
    "selected_indices = [\n",
    "    utterances.get(('seven', 1)),\n",
    "    utterances.get(('seven', 2)),\n",
    "    utterances.get(('four', 1)),\n",
    "    utterances.get(('four', 2))\n",
    "]\n",
    "\n",
    "def process_utterance(wav):\n",
    "    # Compute MFSCs (log Mel spectrogram)\n",
    "    S = librosa.feature.melspectrogram(\n",
    "        y=wav,\n",
    "        sr=16000,\n",
    "        n_fft=int(0.025 * 16000),    # 25 ms window\n",
    "        hop_length=int(0.010 * 16000), # 10 ms hop\n",
    "        n_mels=13\n",
    "    )\n",
    "    log_S = librosa.power_to_db(S)\n",
    "\n",
    "    # Compute MFCCs from the log Mel spectrogram\n",
    "    mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=13)\n",
    "\n",
    "    return log_S, mfcc\n",
    "\n",
    "\n",
    "# Define the number of utterances and the grid layout\n",
    "num_utterances = len(selected_indices)\n",
    "cols = 2  # Number of columns\n",
    "rows = (num_utterances + 1) // cols  # Calculate rows based on number of utterances\n",
    "\n",
    "# Initialize figures for MFSCs and MFCCs\n",
    "fig_mfsc, axes_mfsc = plt.subplots(rows, cols, figsize=(12, 5 * rows))\n",
    "fig_mfcc, axes_mfcc = plt.subplots(rows, cols, figsize=(12, 5 * rows))\n",
    "\n",
    "# Flatten axes for easy indexing\n",
    "axes_mfsc = axes_mfsc.flatten()\n",
    "axes_mfcc = axes_mfcc.flatten()\n",
    "\n",
    "# Loop over each selected utterance\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    if idx is not None:\n",
    "        wav = wavs[idx]\n",
    "        digit = digit_labels[idx]\n",
    "        speaker = speakers[idx]\n",
    "\n",
    "        # Extract MFSCs and MFCCs\n",
    "        mfsc, mfcc = process_utterance(wav)\n",
    "\n",
    "        # Compute correlation matrices\n",
    "        corr_mfsc = np.corrcoef(mfsc)\n",
    "        corr_mfcc = np.corrcoef(mfcc)\n",
    "\n",
    "        # Plot correlation matrix for MFSCs\n",
    "        ax_mfsc = axes_mfsc[i]\n",
    "        ax_mfsc.imshow(corr_mfsc, interpolation='nearest', cmap='inferno')\n",
    "        ax_mfsc.set_title(f'MFSC Correlation\\nDigit: {digit}, Speaker: {speaker}')\n",
    "        ax_mfsc.set_xlabel('MFSC Coefficient Index')\n",
    "        ax_mfsc.set_ylabel('MFSC Coefficient Index')\n",
    "\n",
    "        # Plot correlation matrix for MFCCs\n",
    "        ax_mfcc = axes_mfcc[i]\n",
    "        ax_mfcc.imshow(corr_mfcc, cmap='inferno', interpolation='nearest')\n",
    "        ax_mfcc.set_title(f'MFCC Correlation\\nDigit: {digit}, Speaker: {speaker}')\n",
    "        ax_mfcc.set_xlabel('MFCC Coefficient Index')\n",
    "        ax_mfcc.set_ylabel('MFCC Coefficient Index')\n",
    "    else:\n",
    "        print('Selected index not found.')\n",
    "\n",
    "# Hide any empty subplots\n",
    "for j in range(i + 1, len(axes_mfsc)):\n",
    "    axes_mfsc[j].axis('off')\n",
    "    axes_mfcc[j].axis('off')\n",
    "\n",
    "# Adjust layout for spacing between subplots\n",
    "fig_mfsc.subplots_adjust(hspace=0.4, wspace=0.3)  # Adjust vertical (hspace) and horizontal (wspace) spacing\n",
    "fig_mfcc.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "\n",
    "# Add colorbars and set figure titles\n",
    "fig_mfsc.colorbar(axes_mfsc[0].images[0], ax=axes_mfsc[:i + 1], orientation='vertical', fraction=0.02, pad=0.04)\n",
    "fig_mfcc.colorbar(axes_mfcc[0].images[0], ax=axes_mfcc[:i + 1], orientation='vertical', fraction=0.02, pad=0.04)\n",
    "\n",
    "# Save the combined figures\n",
    "fig_mfsc.savefig('images/corr_mfsc_combined.png')\n",
    "fig_mfcc.savefig('images/corr_mfcc_combined.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Βήμα 5\n",
    "# Step 2: Load data\n",
    "wavs, speakers, digits = data_parser('./data/digits/')\n",
    "\n",
    "# Step 3: Extract MFCCs\n",
    "mfccs = extract_mfccs(wavs)\n",
    "\n",
    "'''\n",
    "Feature Extraction\n",
    "'''\n",
    "def compute_feature_vectors(mfccs):\n",
    "    \"\"\"\n",
    "    Processes audio files in the specified directory to extract feature vectors.\n",
    "\n",
    "    Parameters:\n",
    "    - mfccs (list): List of tuples, each containing MFCCs, deltas, and delta-deltas for an audio file.\n",
    "\n",
    "    Returns:\n",
    "    - feature_vectors (np.ndarray): Array of feature vectors (shape: [num_utterances, 78]).\n",
    "    - labels (list): List of digit labels corresponding to each feature vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 3: Initialize lists to store feature vectors and corresponding labels\n",
    "    feature_vectors = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, (mfcc, delta, delta2) in enumerate(mfccs):\n",
    "        # Concatenate MFCCs, Deltas, and Delta-Deltas along the feature axis\n",
    "        combined_features = np.concatenate((mfcc, delta, delta2), axis=0)  # Shape: (39, frames)\n",
    "        \n",
    "        # Compute mean and standard deviation for each feature across all frames\n",
    "        mean_features = np.mean(combined_features, axis=1)  # Shape: (39,)\n",
    "        std_features = np.std(combined_features, axis=1)    # Shape: (39,)\n",
    "        \n",
    "        # Concatenate mean and std to form a single feature vector (total length: 78)\n",
    "        feature_vector = np.concatenate((mean_features, std_features))  # Shape: (78,)\n",
    "        \n",
    "        # Append to the list of feature vectors\n",
    "        feature_vectors.append(feature_vector)\n",
    "        \n",
    "        # Append the corresponding digit label\n",
    "        labels.append(digits[idx])\n",
    "    \n",
    "    # Convert the list of feature vectors to a NumPy array for easier manipulation\n",
    "    feature_vectors = np.array(feature_vectors)  # Shape: (num_utterances, 78)\n",
    "    \n",
    "    return feature_vectors, labels\n",
    "\n",
    "\n",
    "feature_vectors, labels = compute_feature_vectors(mfccs)\n",
    "\n",
    "'''\n",
    "Prepare for visualization\n",
    "'''\n",
    "\n",
    "def assign_colors_markers(labels):\n",
    "    \"\"\"\n",
    "    Identifies unique digits and assigns unique colors and markers to each digit.\n",
    "    \n",
    "    Parameters:\n",
    "    - labels (list): List of digit labels corresponding to each data point.\n",
    "    \n",
    "    Returns:\n",
    "    - unique_digits (list): Sorted list of unique digit labels.\n",
    "    - digit_to_color (dict): Mapping from each digit to a unique color.\n",
    "    - digit_to_marker (dict): Mapping from each digit to a unique marker.\n",
    "    \"\"\"\n",
    "    # Identify unique digits in the dataset\n",
    "    unique_digits = sorted(set(labels))\n",
    "    \n",
    "    # Assign unique colors and markers to each digit\n",
    "    colors = plt.cm.get_cmap('tab10', len(unique_digits))\n",
    "    markers = ['o', 's', '^', 'D', 'v', '<', '>', 'p', '*', 'h']  # Extend or modify as needed\n",
    "    \n",
    "    digit_to_color = {digit: colors(idx) for idx, digit in enumerate(unique_digits)}\n",
    "    digit_to_marker = {digit: markers[idx % len(markers)] for idx, digit in enumerate(unique_digits)}\n",
    "    \n",
    "    return unique_digits, digit_to_color, digit_to_marker\n",
    "\n",
    "\n",
    "unique_digits, digit_to_color, digit_to_marker = assign_colors_markers(labels)\n",
    "\n",
    "'''\n",
    "Scatterplot of feature vectors\n",
    "'''\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for idx, feature_vector in enumerate(feature_vectors):\n",
    "    digit = labels[idx]\n",
    "    color = digit_to_color[digit]\n",
    "    marker = digit_to_marker[digit]\n",
    "    \n",
    "    # Extract the first two dimensions (mean of the first two features)\n",
    "    x = feature_vector[0]  # Mean of first feature\n",
    "    y = feature_vector[1]  # Mean of second feature\n",
    "    \n",
    "    # Plot the point\n",
    "    plt.scatter(x, y, color=color, marker=marker, label=digit, edgecolors='k', s=100)\n",
    "\n",
    "# Create custom legend to avoid duplicate labels\n",
    "handles = []\n",
    "for digit in unique_digits:\n",
    "    handles.append(plt.Line2D([], [], color=digit_to_color[digit], marker=digit_to_marker[digit],\n",
    "                              linestyle='', markersize=10, label=digit))\n",
    "\n",
    "plt.legend(handles=handles, title='Digits')\n",
    "plt.title('Scatter Plot of Feature Vectors (First Two Dimensions)')\n",
    "plt.xlabel('Mean for MFCC No. 1')\n",
    "plt.ylabel('Variance for MFCC No. 1')\n",
    "plt.grid(True)\n",
    "# Save the plot\n",
    "plt.savefig('images/feature_vectors_scatterplot.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Βήμα 6\n",
    "import matplotlib.pyplot as plt\n",
    "from step2 import data_parser\n",
    "from step3 import extract_mfccs\n",
    "from step5 import compute_feature_vectors, assign_colors_markers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 2: Load data\n",
    "wavs, speakers, digits = data_parser('./data/digits/')\n",
    "\n",
    "# Step 3: Extract MFCCs\n",
    "mfccs = extract_mfccs(wavs)\n",
    "\n",
    "# Step 5: Compute feature vectors\n",
    "feature_vectors, labels = compute_feature_vectors(mfccs)\n",
    "\n",
    "'''\n",
    "PCA Variance Retained\n",
    "'''\n",
    "\n",
    "def create_scaling_pca_pipeline(n_components=2):\n",
    "    \"\"\"\n",
    "    Creates a scikit-learn Pipeline that scales the data and then applies PCA.\n",
    "\n",
    "    Parameters:\n",
    "    - n_components (int): Number of principal components to retain.\n",
    "\n",
    "    Returns:\n",
    "    - pipeline (sklearn.pipeline.Pipeline): The scaling and PCA pipeline.\n",
    "    \"\"\"\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=n_components))\n",
    "    ])\n",
    "    return pipeline\n",
    "\n",
    "# Initialize PCA with 2 components\n",
    "pca_2 = create_scaling_pca_pipeline(n_components=2)\n",
    "\n",
    "# Fit PCA on the feature vectors and transform the data\n",
    "principal_components_2 = pca_2.fit_transform(feature_vectors)\n",
    "\n",
    "# Retrieve the percentage of variance explained by each principal component from the pipeline\n",
    "variance_2 = pca_2.named_steps['pca'].explained_variance_ratio_\n",
    "\n",
    "print(f\"Variance Retained by the first two principal components: {variance_2.sum()*100:.2f}%\")\n",
    "\n",
    "'''\n",
    "PCA 2D Plot\n",
    "'''\n",
    "\n",
    "# Assign colors and markers to each digit\n",
    "unique_digits, digit_to_color, digit_to_marker = assign_colors_markers(labels)\n",
    "\n",
    "\n",
    "# Initialize the plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for idx, (pc1, pc2) in enumerate(principal_components_2):\n",
    "    digit = labels[idx]\n",
    "    color = digit_to_color[digit]\n",
    "    marker = digit_to_marker[digit]\n",
    "    plt.scatter(pc1, pc2, color=color, marker=marker, s=100, edgecolors='k')\n",
    "\n",
    "# Create custom legend to avoid duplicate labels\n",
    "handles = []\n",
    "for digit in unique_digits:\n",
    "    handles.append(plt.Line2D([], [], color=digit_to_color[digit], marker=digit_to_marker[digit],\n",
    "                              linestyle='', markersize=10, label=digit))\n",
    "\n",
    "plt.legend(handles=handles, title='Digits')\n",
    "plt.title('PCA Scatter Plot (2 Dimensions)')\n",
    "plt.xlabel(f'Principal Component 1 ({variance_2[0]*100:.2f}% Variance)')\n",
    "plt.ylabel(f'Principal Component 2 ({variance_2[1]*100:.2f}% Variance)')\n",
    "plt.grid(True)\n",
    "# Save the plot\n",
    "plt.savefig('images/pca_2d_plot.png')\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "PCA 3D Plot\n",
    "'''\n",
    "\n",
    "# Initialize PCA with 3 components\n",
    "pca_3 = create_scaling_pca_pipeline(n_components=3)\n",
    "\n",
    "# Fit PCA on the feature vectors and transform the data\n",
    "principal_components_3 = pca_3.fit_transform(feature_vectors)\n",
    "\n",
    "# Retrieve the percentage of variance explained by each principal component\n",
    "variance_3 = pca_3.named_steps['pca'].explained_variance_ratio_\n",
    "\n",
    "print(f\"Variance Retained by the first three principal components: {variance_3.sum()*100:.2f}%\")\n",
    "\n",
    "# Plot the 3D scatter plot\n",
    "\n",
    "# Initialize the plot\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create 3D axes\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "for idx, (pc1, pc2, pc3) in enumerate(principal_components_3):\n",
    "    digit = labels[idx]\n",
    "    color = digit_to_color[digit]\n",
    "    marker = digit_to_marker[digit]\n",
    "    ax.scatter(pc1, pc2, pc3, color=color, marker=marker, s=100, edgecolors='k')\n",
    "    \n",
    "# Create custom legend to avoid duplicate labels\n",
    "handles = []\n",
    "for digit in unique_digits:\n",
    "    handles.append(plt.Line2D([], [], color=digit_to_color[digit], marker=digit_to_marker[digit],\n",
    "                              linestyle='', markersize=10, label=digit))\n",
    "    \n",
    "ax.legend(handles=handles, title='Digits')\n",
    "ax.set_title('PCA Scatter Plot (3 Dimensions)')\n",
    "ax.set_xlabel(f'Principal Component 1 ({variance_3[0]*100:.2f}% Variance)')\n",
    "ax.set_ylabel(f'Principal Component 2 ({variance_3[1]*100:.2f}% Variance)')\n",
    "ax.set_zlabel(f'Principal Component 3 ({variance_3[2]*100:.2f}% Variance)')\n",
    "# Save the plot\n",
    "plt.savefig('images/pca_3d_plot.png')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import librosa\n",
    "from plot_confusion_matrix import plot_confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "# import one-hot encoder\n",
    "\n",
    "from step2 import data_parser\n",
    "from step3 import extract_mfccs\n",
    "from step5 import assign_colors_markers\n",
    "from step5 import compute_feature_vectors\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Step 2: Load data\n",
    "wavs, speakers, digits = data_parser('./data/digits/')\n",
    "\n",
    "# Step 3: Extract MFCCs\n",
    "mfccs = extract_mfccs(wavs)\n",
    "\n",
    "# Step 5: Compute feature vectors\n",
    "feature_vectors, labels = compute_feature_vectors(mfccs)\n",
    "\n",
    "unique_digits, digit_to_color, digit_to_marker = assign_colors_markers(labels)\n",
    "\n",
    "# Step 7: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, labels, test_size=0.3, random_state=42, stratify=labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['five',\n",
       " 'eight',\n",
       " 'six',\n",
       " 'three',\n",
       " 'six',\n",
       " 'two',\n",
       " 'nine',\n",
       " 'five',\n",
       " 'one',\n",
       " 'nine',\n",
       " 'six',\n",
       " 'five',\n",
       " 'two',\n",
       " 'one',\n",
       " 'six',\n",
       " 'two',\n",
       " 'five',\n",
       " 'three',\n",
       " 'one',\n",
       " 'nine',\n",
       " 'three',\n",
       " 'six',\n",
       " 'five',\n",
       " 'four',\n",
       " 'four',\n",
       " 'five',\n",
       " 'one',\n",
       " 'two',\n",
       " 'nine',\n",
       " 'seven',\n",
       " 'six',\n",
       " 'six',\n",
       " 'four',\n",
       " 'two',\n",
       " 'two',\n",
       " 'seven',\n",
       " 'eight',\n",
       " 'seven',\n",
       " 'seven',\n",
       " 'three',\n",
       " 'six',\n",
       " 'three',\n",
       " 'three',\n",
       " 'nine',\n",
       " 'nine',\n",
       " 'four',\n",
       " 'seven',\n",
       " 'five',\n",
       " 'four',\n",
       " 'five',\n",
       " 'three',\n",
       " 'two',\n",
       " 'seven',\n",
       " 'seven',\n",
       " 'eight',\n",
       " 'seven',\n",
       " 'one',\n",
       " 'three',\n",
       " 'eight',\n",
       " 'four',\n",
       " 'four',\n",
       " 'three',\n",
       " 'seven',\n",
       " 'eight',\n",
       " 'four',\n",
       " 'five',\n",
       " 'nine',\n",
       " 'four',\n",
       " 'one',\n",
       " 'three',\n",
       " 'two',\n",
       " 'eight',\n",
       " 'one',\n",
       " 'six',\n",
       " 'one',\n",
       " 'nine',\n",
       " 'five',\n",
       " 'eight',\n",
       " 'four',\n",
       " 'five',\n",
       " 'eight',\n",
       " 'eight',\n",
       " 'eight',\n",
       " 'three',\n",
       " 'one',\n",
       " 'six',\n",
       " 'nine',\n",
       " 'two',\n",
       " 'one',\n",
       " 'nine',\n",
       " 'two',\n",
       " 'seven',\n",
       " 'four']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "CustomBayesClassifier\n",
    "'''\n",
    "\n",
    "class CustomBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.classes = None\n",
    "        self.priors = {}\n",
    "        self.mean = {}\n",
    "        self.var = {}\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the Bayesian classifier according to X, y.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (np.ndarray): Training feature vectors (num_samples, num_features).\n",
    "        - y (list): Training labels.\n",
    "        \"\"\"\n",
    "        self.classes = np.unique(y)\n",
    "        for cls in self.classes:\n",
    "            X_c = X[np.array(y) == cls]\n",
    "            self.priors[cls] = X_c.shape[0] / X.shape[0]\n",
    "            self.mean[cls] = np.mean(X_c, axis=0)\n",
    "            self.var[cls] = np.var(X_c, axis=0) + 1e-9  # Add small value to prevent division by zero (var_smoothing)\n",
    "    \n",
    "    def _gaussian_log_prob(self, cls, x):\n",
    "        \"\"\"\n",
    "        Calculate the log probability of x for class cls using Gaussian distribution.\n",
    "        \n",
    "        Parameters:\n",
    "        - cls (str): Class label.\n",
    "        - x (np.ndarray): Feature vector.\n",
    "        \n",
    "        Returns:\n",
    "        - log_prob (float): Log probability.\n",
    "        \"\"\"\n",
    "        mean = self.mean[cls]\n",
    "        var = self.var[cls]\n",
    "        # Calculate log Gaussian probability\n",
    "        log_prob = -0.5 * np.sum(np.log(2. * np.pi * var))\n",
    "        log_prob -= 0.5 * np.sum(((x - mean) ** 2) / var)\n",
    "        return log_prob\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform classification on an array of test vectors X.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (np.ndarray): Test feature vectors (num_samples, num_features).\n",
    "        \n",
    "        Returns:\n",
    "        - predictions (list): Predicted class labels.\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            class_probs = {}\n",
    "            for cls in self.classes:\n",
    "                # Compute log prior + log likelihood\n",
    "                class_probs[cls] = np.log(self.priors[cls]) + self._gaussian_log_prob(cls, x)\n",
    "            # Select the class with the highest probability\n",
    "            predicted_class = max(class_probs, key=class_probs.get)\n",
    "            predictions.append(predicted_class)\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "        \n",
    "        Parameters:\n",
    "        - X (np.ndarray): Test feature vectors (num_samples, num_features).\n",
    "        - y (list): True class labels.\n",
    "        \n",
    "        Returns:\n",
    "        - score (float): Mean accuracy.\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        return accuracy_score(y, predictions)\n",
    "\n",
    "\n",
    "'''\n",
    "Comparison with Gaussian Naive Bayes\n",
    "'''\n",
    "\n",
    "clfs = {\n",
    "    'Custom Bayesian': CustomBayesClassifier(),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'SVM': SVC(kernel='linear', random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'CatBoost': CatBoostClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "# Build pipelines for each classifier\n",
    "pipelines = {}\n",
    "\n",
    "for clf_name, clf in clfs.items():\n",
    "    pipelines[clf_name] = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('classifier', clf)\n",
    "    ])\n",
    "    \n",
    "# Keep resutls for each classifier in order to compare them\n",
    "results = {}\n",
    "    \n",
    "# Train and evaluate each classifier\n",
    "for clf_name, pipeline in pipelines.items():\n",
    "    print(f\"\\n{clf_name} Classifier\")\n",
    "    # Train\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    # Get f1 score\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=unique_digits)\n",
    "    plot_confusion_matrix(conf_matrix, classes=unique_digits, title=f'Confusion Matrix - {clf_name}', normalize=True)\n",
    "    # Classification report\n",
    "    print(classification_report(y_test, y_pred, target_names=unique_digits))\n",
    "    \n",
    "    results[clf_name] = {'accuracy': accuracy, 'f1': f1}\n",
    "    \n",
    "# Compare classifiers based on accuracy and f1 score\n",
    "print(\"\\nComparison of Classifiers:\")\n",
    "\n",
    "# Get the best classifier based on accuracy\n",
    "best_accuracy = max(results, key=lambda x: results[x]['accuracy'])\n",
    "print(f\"Best Classifier based on Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Get the best classifier based on f1 score\n",
    "best_f1 = max(results, key=lambda x: results[x]['f1'])\n",
    "print(f\"Best Classifier based on F1 Score: {best_f1}\")\n",
    "\n",
    "'''\n",
    "Bonus\n",
    "'''\n",
    "\n",
    "def compute_feature_vectors_enhanced(mfccs, digits, wavs):\n",
    "    \"\"\"\n",
    "    Processes audio files to extract enhanced feature vectors including MFCCs, deltas, delta-deltas,\n",
    "    Zero-Crossing Rate (ZCR), and Spectral Centroid.\n",
    "    \n",
    "    Parameters:\n",
    "    - mfccs (list): List of tuples, each containing MFCCs, deltas, and delta-deltas for an audio file.\n",
    "    - digits (list): List of digit labels corresponding to each audio file.\n",
    "    - wavs (list): List of waveform data for each audio file.\n",
    "    \n",
    "    Returns:\n",
    "    - feature_vectors (np.ndarray): Array of enhanced feature vectors.\n",
    "    - labels (list): List of digit labels corresponding to each feature vector.\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_vectors = []\n",
    "    labels = []\n",
    "    \n",
    "    for idx, ((mfcc, delta, delta2), wav) in enumerate(zip(mfccs, wavs)):\n",
    "        # Concatenate MFCCs, Deltas, and Delta-Deltas\n",
    "        combined_features = np.concatenate((mfcc, delta, delta2), axis=0)  # Shape: (39, frames)\n",
    "        \n",
    "        # Compute mean and standard deviation for each feature across all frames\n",
    "        mean_features = np.mean(combined_features, axis=1)  # Shape: (39,)\n",
    "        std_features = np.std(combined_features, axis=1)    # Shape: (39,)\n",
    "        \n",
    "        # Compute Zero-Crossing Rate (ZCR)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=wav)\n",
    "        zcr_mean = np.mean(zcr)\n",
    "        zcr_std = np.std(zcr)\n",
    "        \n",
    "        # Compute Spectral Centroid\n",
    "        spectral_centroid = librosa.feature.poly_features(y=wav, sr=16000, hop_length=20, win_length=25, order=3)\n",
    "        spectral_centroid_mean = np.mean(spectral_centroid)\n",
    "        spectral_centroid_std = np.std(spectral_centroid)\n",
    "        \n",
    "        # Concatenate all features into a single vector\n",
    "        feature_vector = np.concatenate((\n",
    "            mean_features,       # 39\n",
    "            std_features,        # 39\n",
    "            [zcr_mean, zcr_std], # 2\n",
    "            [spectral_centroid_mean, spectral_centroid_std] # 2\n",
    "        ))  # Total length: 39 + 39 + 2 + 2 = 82\n",
    "        \n",
    "        feature_vectors.append(feature_vector)\n",
    "        labels.append(digits[idx])\n",
    "    \n",
    "    feature_vectors = np.array(feature_vectors)\n",
    "    return feature_vectors, labels\n",
    "\n",
    "\n",
    "# Step 5: Compute feature vectors\n",
    "feature_vectors, labels = compute_feature_vectors_enhanced(mfccs, digits, wavs)\n",
    "\n",
    "# Step 7: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_vectors, labels, test_size=0.3, random_state=42, stratify=labels)\n",
    "\n",
    "for clf_name, pipeline in pipelines.items():\n",
    "    print(f\"\\n{clf_name} Classifier\")\n",
    "    # Train\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    # Evaluate\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "    # Get f1 score\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
    "    # Confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=unique_digits)\n",
    "    plot_confusion_matrix(conf_matrix, classes=unique_digits, title=f'Confusion Matrix - {clf_name}', normalize=True)\n",
    "    # Classification report\n",
    "    print(classification_report(y_test, y_pred, target_names=unique_digits))\n",
    "    \n",
    "    results[clf_name] = {'accuracy': accuracy, 'f1': f1}\n",
    "    \n",
    "    \n",
    "# Compare classifiers based on accuracy and f1 score\n",
    "print(\"\\nComparison of Classifiers:\")\n",
    "\n",
    "# Get the best classifier based on accuracy\n",
    "best_accuracy = max(results, key=lambda x: results[x]['accuracy'])\n",
    "print(f\"Best Classifier based on Accuracy: {best_accuracy}\")\n",
    "\n",
    "# Get the best classifier based on f1 score\n",
    "best_f1 = max(results, key=lambda x: results[x]['f1'])\n",
    "print(f\"Best Classifier based on F1 Score: {best_f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "potam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
