\documentclass[a4paper,12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[LGR,T1]{fontenc}
\usepackage{alphabeta}
\usepackage{amsmath}
\usepackage{float}
\usepackage{multirow}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{booktabs}  % For better looking tables
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{breakurl}
\usepackage[numbers]{natbib}

\lstset{
  basicstyle=\ttfamily\footnotesize,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{red},
  showstringspaces=false,
  breaklines=true,
  frame=single,
  language=R,
  extendedchars=true,
  literate={β}{{\beta}}1
}

\title{Αναγνώριση Προτύπων \\ 1η Εργαστηριακή Άσκηση \\ Χειμερινό Εξάμηνο 2024-2025 \\ Ε.ΔE.ΜΜ}
\author{Σπανάκης Παναγιώτης-Αλέξιος (ΑΜ: 03400274)}
\date{08/11/2024}

\begin{document}

\maketitle
\section*{Βήμα 1}

\begin{table}[h!]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        \textbf{Vowel}      & \textbf{Speaker} & \textbf{Pitch (Hz)} & \textbf{F1 (Hz)} & \textbf{F2 (Hz)} & \textbf{F3 (Hz)} \\ \hline
        \multirow{2}{*}{α}  & man              & 134.06              & 777.91           & 1217.69          & 2405.49          \\ \cline{2-6}
                            & woman            & 176.84              & 859.08           & 1837.31          & 3146.39          \\ \hline
        \multirow{2}{*}{ου} & man              & 130.65              & 372.17           & 1788.73          & 2327.56          \\ \cline{2-6}
                            & woman            & 184.51              & 321.15           & 1566.12          & 2631.54          \\ \hline
        \multirow{2}{*}{ι}  & man              & 132.04              & 387.10           & 2047.69          & 2556.83          \\ \cline{2-6}
                            & woman            & 178.49              & 368.76           & 2259.41          & 2951.24          \\ \hline
    \end{tabular}
    \caption{Measurements for vowels by male and female speakers.}
    \label{tab:vowels}
\end{table}

\textbf{Αρχικά, Σύγκριση Θεμελιώδους Συχνότητας (Pitch):}
Όπως αναμενόταν, η μέση συχνότητα της γυναικείας ομιλήτριας (Ομιλητής 2) είναι αισθητά υψηλότερη από εκείνη του ανδρικού ομιλητή (Ομιλητής 1)
για όλα τα φωνήεντα. Συγκεκριμένα, για το φωνήεν \textbf{"α"} ο άντρας έχει 134 Hz ενώ η γυναίκα 177 Hz, για το \textbf{"ου"} ο
άντρας έχει 131 Hz ενώ η γυναίκα 185 Hz, και για το \textbf{"ι"} ο άντρας 132 Hz έναντι της γυναίκας στα 178 Hz.
Αυτή η διαφορά είναι σύμφωνη με τις γενικές φωνητικές τάσεις όπου οι γυναίκες έχουν υψηλότερες συχνότητες λόγω ανατομικών διαφορών,
όπως οι μικρότερες και πιο τεντωμένες φωνητικές χορδές.

\textbf{Παρατηρήσεις για τα Formants:}
Οι τιμές των μορφάντων (F1, F2, F3) είναι επίσης υψηλότερες για τη γυναίκα ομιλήτρια συγκριτικά με τον άντρα,
κάτι που αναμένεται λόγω του μικρότερου φωνητικού σωλήνα, ο οποίος επηρεάζει τις συχνότητες αντήχησης και οδηγεί σε
μεγαλύτερες τιμές για τα formants. Για παράδειγμα, για το φωνήεν \textbf{"α"}, η πρώτη μορφάντις (F1) του άνδρα είναι
778 Hz, ενώ της γυναίκας είναι 859 Hz. Παρόμοια, η δεύτερη μορφάντις (F2) του άνδρα είναι 1218 Hz ενώ της γυναίκας
1837 Hz, και η τρίτη (F3) του άνδρα είναι 2405 Hz έναντι 3146 Hz για τη γυναίκα. Το ίδιο μοτίβο παρατηρείται και στα υπόλοιπα φωνήεντα.

\textbf{Ειδικές Παρατηρήσεις ανά Φωνήεν:}
\begin{itemize}
    \item Για το \textbf{"α"}, παρατηρούμε υψηλότερη F1 και F2 και στους δύο ομιλητές σε σχέση με τα φωνήεντα "ου" και "ι", κάτι που δείχνει ότι το "α" είναι πιο ανοιχτό και κεντρικό φωνήεν. Οι σημαντικά υψηλότερες τιμές F2 και F3 για τη γυναίκα ομιλήτρια πιθανώς υποδεικνύουν μια πιο πρόσθια εκφορά.
    \item Το \textbf{"ου"} εμφανίζει τη χαμηλότερη F1 και στους δύο ομιλητές, κάτι που δείχνει πως πρόκειται για ένα πιο κλειστό και οπίσθιο φωνήεν. Η διαφορά της F2 ανάμεσα στους ομιλητές ενισχύει την οπίσθια θέση του φωνήεντος και για τα δύο φύλα.
    \item Για το \textbf{"ι"}, παρατηρείται υψηλή F2, ειδικά για τη γυναίκα, κάτι που συνάδει με τον πρόσθιο, υψηλό χαρακτήρα του φωνήεντος αυτού. Η κοντινή απόσταση μεταξύ F2 και F3 στη γυναικεία φωνή μπορεί να υποδηλώνει μια πιο έντονη εκφορά.
\end{itemize}

\textbf{Γενικές Διαφορές μεταξύ των Φύλων:}
Οι διαφορές στις συχνότητες της θεμελιώδους συχνότητας και των μορφάντων που παρατηρήθηκαν είναι σύμφωνες με τις κοινές φωνητικές διακρίσεις μεταξύ ανδρικών και γυναικείων φωνών, με τον ανδρικό ομιλητή να εμφανίζει χαμηλότερες συχνότητες σε όλες τις παραμέτρους.

\section*{Βήμα 2}

Η συνάρτηση \texttt{data\_parser(directory)} λειτουργεί ως εξής:

\begin{enumerate}
    \item \textbf{Αρχικοποίηση}: Δημιουργεί τρεις κενές λίστες (\texttt{wavs}, \texttt{speakers}, \texttt{digits}).
    \item \textbf{Διάβασμα Αρχείων}: Διατρέχει όλα τα αρχεία στον καθορισμένο κατάλογο.
    \item \textbf{Φόρτωση Ήχου}: Φορτώνει κάθε αρχείο ήχου με συχνότητα 16kHz και προσθέτει τα δεδομένα στη λίστα \texttt{wavs}.
    \item \textbf{Εξαγωγή Πληροφοριών}: Από το όνομα του αρχείου, χρησιμοποιεί κανονική έκφραση για να εξάγει το ψηφίο και τον αριθμό του ομιλητή, τα οποία προστίθενται στις λίστες \texttt{digits} και \texttt{speakers} αντίστοιχα.
    \item \textbf{Επιστροφή Αποτελεσμάτων}: Επιστρέφει τις τρεις λίστες με τα ηχητικά δεδομένα, τους ομιλητές και τα ψηφία.
\end{enumerate}

\section*{Βήμα 3}

Η συνάρτηση \texttt{extract\_mfccs(wavs)} λειτουργεί ως εξής:

\begin{enumerate}
    \item \textbf{Αρχικοποίηση}: Δημιουργεί μια κενή λίστα \texttt{mfccs} για την αποθήκευση των χαρακτηριστικών.
    \item \textbf{Εξαγωγή MFCCs}: Για κάθε ήχο στη λίστα \texttt{wavs}, εξάγει 13 Mel-Frequency Cepstral Coefficients (MFCCs) χρησιμοποιώντας παράθυρο μήκους 25 ms και βήμα 10 ms (\texttt{n\_fft=400}, \texttt{hop\_length=160}).
    \item \textbf{Υπολογισμός Deltas}: Υπολογίζει την πρώτη (\texttt{delta}) και δεύτερη (\texttt{delta2}) τοπική παράγωγο των MFCCs.
    \item \textbf{Αποθήκευση Χαρακτηριστικών}: Προσθέτει τα MFCCs, τα deltas και τα delta-deltas ως ένα tuple στη λίστα \texttt{mfccs}.
    \item \textbf{Επιστροφή Αποτελεσμάτων}: Επιστρέφει τη λίστα \texttt{mfccs} με τα εξαγόμενα χαρακτηριστικά για κάθε αρχείο ήχου.
\end{enumerate}

\section*{Βήμα 4}

Τα ψηφία που αναλύθηκαν είναι $n1=7$ και $n2=4$ του 1ου και 2ου MFCC αντίστοιχα. Τα ιστογράμματα των ψηφίων αυτών για τα πρώτα 2 MFCCs παρουσιάζονται στα
Σχήματα \ref{fig:hist1} και \ref{fig:hist2}. Η διακύμανση των MFCCs για τα δύο ψηφία φαίνεται στον πίνακα \ref{tab:variance}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/histograms_seven.png}
    \caption{Ιστόγραμμα του ψηφίου "seven" για τους δύο MFCCs.}
    \label{fig:hist1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{images/histograms_four.png}
    \caption{Ιστόγραμμα του ψηφίου "four" για τους δύο MFCCs.}
    \label{fig:hist2}
\end{figure}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Digit} & \textbf{MFCC 1} & \textbf{MFCC 2} \\ \hline
        seven          & 21497.80        & 3068.08         \\ \hline
        four           & 14009.79        & 4906.44         \\ \hline
    \end{tabular}
    \caption{Variance of MFCCs for digits "seven" and "four".}
    \label{tab:variance}
\end{table}

Mπορούμε να εξάγουμε τα εξής συμπεράσματα:

Πρώτον, παρατηρείται σημαντική διαφορά στη διακύμανση των δύο πρώτων MFCC χαρακτηριστικών μεταξύ των ψηφίων "seven" και "four".
Συγκεκριμένα, το MFCC1 του "seven" παρουσιάζει πολύ μεγαλύτερη διακύμανση σε σύγκριση με το "four" (21497.80 έναντι 14008.79),
ενώ η διακύμανση του MFCC2 παρουσιάζει επίσης διαφορά, αν και μικρότερη (3068.08 για το "seven" και 4096.44 για το "four").

Δεύτερον, τα ιστογράμματα των MFCC1 και MFCC2 δείχνουν ότι οι κατανομές των δύο ψηφίων διαφέρουν αρκετά,
ειδικά για το MFCC1, όπου το "seven" εμφανίζει υψηλότερες τιμές στην κατανομή του σε σύγκριση με το "four".
Το MFCC2, αν και εμφανίζει λιγότερο έντονες διαφορές, διατηρεί διακριτό μοτίβο ανάμεσα στα δύο ψηφία,
το οποίο μπορεί να χρησιμοποιηθεί για την αναγνώρισή τους.

Συμπερασματικά, η διαφορά στη διακύμανση και στην κατανομή των MFCC χαρακτηριστικών υποδηλώνει ότι τα MFCC1 και
MFCC2 μπορούν να χρησιμοποιηθούν για τη διάκριση των ψηφίων "seven" και "four" σε ένα classification σύστημα.
Οι διαφορετικές κατανομές δείχνουν ότι υπάρχουν χαρακτηριστικά στις συχνότητες των εκφωνήσεων που
διαχωρίζουν τα δύο ψηφία, γεγονός που ενισχύει την απόδοση σε εφαρμογές αναγνώρισης ομιλίας.

Για την ανάλυση των χαρακτηριστικών των ψηφίων "seven" και "four" από δύο διαφορετικούς ομιλητές,
εξάγαμε και αναπαραστήσαμε τις συσχετίσεις των Mel Filterbank Spectral Coefficients (MFSCs) και των Mel-Frequency Cepstral Coefficients (MFCCs).
Τα αποτελέσματα παρουσιάζονται στα διαγράμματα που φαίνονται στις Εικόνες~\ref{fig:mfsc_correlation} και~\ref{fig:mfcc_correlation}.

Στην Εικόνα~\ref{fig:mfsc_correlation}, παρατηρείται ότι οι συσχετίσεις μεταξύ των MFSCs είναι υψηλές και ομοιόμορφες,
ιδιαίτερα στα κατώτερα MFSCs, όπου εμφανίζεται έντονη συσχέτιση. Αυτό υποδηλώνει ότι τα MFSCs
έχουν μεγαλύτερη εξάρτηση μεταξύ των συστατικών τους, κάτι που μπορεί να οφείλεται στις ακατέργαστες
συχνότητες χωρίς την εφαρμογή του μετασχηματισμού DCT. Η ομοιόμορφη κατανομή των υψηλών συσχετίσεων
δείχνει ότι τα MFSCs περιέχουν πλεονάζουσες πληροφορίες, οι οποίες μπορεί να επηρεάσουν την
αποτελεσματικότητα ενός classification συστήματος, καθώς δεν διαχωρίζονται εύκολα τα χρήσιμα χαρακτηριστικά από τον θόρυβο.
Γενικότερα, για την εκπαίδευση των ταξινομητών, τα χαρακτηριστικά που χρησιμοποιούνται πρέπει να είναι ανεξάρτητα και
να μη συσχετίζονται μεταξύ τους, κάτι που δεν παρατηρείται στα MFSCs.

Αντίθετα, η Εικόνα~\ref{fig:mfcc_correlation} δείχνει τις συσχετίσεις των MFCCs, όπου παρατηρείται ότι
οι συσχετίσεις είναι πιο ποικίλες, με χαμηλότερη ομοιομορφία και λιγότερη εξάρτηση μεταξύ των χαρακτηριστικών.
Αυτό οφείλεται στη χρήση του μετασχηματισμού DCT, ο οποίος μειώνει τη συσχέτιση μεταξύ των MFCCs,
διατηρώντας κυρίως τα σημαντικά φασματικά χαρακτηριστικά και αφαιρώντας τον θόρυβο. Έτσι, τα
MFCCs αποτελούν πιο διακριτά χαρακτηριστικά που διευκολύνουν τη διάκριση μεταξύ των ψηφίων και των διαφορετικών ομιλητών.

Συμπερασματικά, η χρήση των MFCCs αντί των MFSCs είναι προτιμότερη σε συστήματα αναγνώρισης ομιλίας,
καθώς τα MFCCs προσφέρουν καλύτερη διακριτικότητα και μικρότερη πλεοναστικότητα στις πληροφορίες.
Ο μετασχηματισμός DCT βοηθά στην απομάκρυνση των ανεπιθύμητων συσχετίσεων και στη διατήρηση των
σημαντικών φωνητικών χαρακτηριστικών, γεγονός που οδηγεί σε καλύτερη απόδοση των ταξινομητικών συστημάτων.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/corr_mfsc_combined.png}
    \caption{Συσχέτιση MFSCs για τα ψηφία "seven" και "four" από δύο ομιλητές.}
    \label{fig:mfsc_correlation}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/corr_mfcc_combined.png}
    \caption{Συσχέτιση MFCCs για τα ψηφία "seven" και "four" από δύο ομιλητές.}
    \label{fig:mfcc_correlation}
\end{figure}

\section*{Βήμα 5}

Για την προσέγγιση αναγνώρισης ψηφίων, πραγματοποιήθηκε η εξαγωγή ενός μοναδικού διανύσματος χαρακτηριστικών για κάθε εκφώνηση. Συγκεκριμένα, τα χαρακτηριστικά MFCCs, οι πρώτες διαφορές (deltas) και οι δεύτερες διαφορές (delta-deltas) συνδυάστηκαν σε ένα ενιαίο σύνολο χαρακτηριστικών. Για κάθε εκφώνηση υπολογίστηκε η μέση τιμή και η τυπική απόκλιση κάθε χαρακτηριστικού σε όλα τα παράθυρα της εκφώνησης, ώστε να προκύψει ένα διάνυσμα με συνολικά 78 διαστάσεις για κάθε εκφώνηση.

Στη συνέχεια, απεικονίστηκαν οι πρώτες δύο διαστάσεις αυτών των διανυσμάτων χαρακτηριστικών (δηλαδή η μέση τιμή και διακύμανση του πρώτου χαρακτηριστικού) σε ένα διάγραμμα διασποράς (scatter plot), όπου κάθε ψηφίο αναπαρίσταται με διαφορετικό χρώμα και σύμβολο (βλ. Εικόνα~\ref{fig:scatter_plot_feature_vectors}).

\textbf{Σχολιασμός Διαγράμματος}

Από το διάγραμμα παρατηρούμε ότι τα διαφορετικά ψηφία σχηματίζουν ξεχωριστά clusters στο χώρο των χαρακτηριστικών, κάτι που δείχνει ότι τα διανύσματα χαρακτηριστικών διαχωρίζονται επαρκώς για κάθε ψηφίο. Υπάρχουν, βέβαια, κάποιες αλληλοεπικαλύψεις μεταξύ ορισμένων ψηφίων, γεγονός που υποδηλώνει ότι ορισμένα ψηφία έχουν πιο όμοια φασματικά χαρακτηριστικά και μπορεί να χρειαστεί περαιτέρω επεξεργασία ή βελτιστοποίηση για την πλήρη διάκρισή τους.

Η διακριτότητα των clusters υποδηλώνει ότι τα MFCCs, μαζί με τις πρώτες και δεύτερες διαφορές τους,
αποτελούν αποτελεσματικά χαρακτηριστικά για την αναγνώριση ομιλίας και τον διαχωρισμό των ψηφίων. Επομένως,
η παρούσα προσέγγιση μπορεί να χρησιμοποιηθεί ως αρχική βάση για την ταξινόμηση των ψηφίων,
με πιθανή βελτίωση μέσω επιπρόσθετων τεχνικών ή ταξινομητών.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/feature_vectors_scatterplot.png}
    \caption{Scatter Plot των διανυσμάτων χαρακτηριστικών (Πρώτες δύο διαστάσεις) για τα ψηφία "eight", "five", "four", "nine", "one", "seven", "six", "three" και "two". Κάθε ψηφίο αναπαρίσταται με διαφορετικό χρώμα και σύμβολο.}
    \label{fig:scatter_plot_feature_vectors}
\end{figure}

\section*{Βήμα 6}

Για την απεικόνιση των πολυδιάστατων διανυσμάτων χαρακτηριστικών που δημιουργήθηκαν από τα MFCCs, εφαρμόσαμε μείωση διαστάσεων μέσω Principal Component Analysis (PCA). Αρχικά, μειώσαμε τα διανύσματα σε δύο διαστάσεις και δημιουργήσαμε ένα δισδιάστατο διάγραμμα διασποράς (scatter plot), ενώ στη συνέχεια μειώσαμε σε τρεις διαστάσεις και δημιουργήσαμε ένα τρισδιάστατο scatter plot.

Σύμφωνα με τα αποτελέσματα που προέκυψαν από την ανάλυση PCA,
οι δύο πρώτες κύριες συνιστώσες διατηρούν 20.07\%
της αρχικής διασποράς των δεδομένων (11.21\% και 8.86\% για την πρώτη και δεύτερη συνιστώσα αντίστοιχα),
ενώ οι τρεις πρώτες κύριες συνιστώσες διατηρούν **28.09\% της διασποράς (11.21\%, 8.86\%, και 8.02\% για
την πρώτη, δεύτερη και τρίτη συνιστώσα αντίστοιχα). Η πληροφορία αυτή μας δείχνει ότι οι πρώτες τρεις
συνιστώσες καταφέρνουν να συγκρατήσουν σημαντικό ποσοστό της αρχικής διασποράς, αν και όχι τόσο μεγάλο ώστε να εξηγούν πλήρως τα δεδομένα.

\begin{enumerate}
    \item Στο δισδιάστατο διάγραμμα (Εικόνα~\ref{fig:pca_2d_plot}), τα σημεία που αντιπροσωπεύουν τα διαφορετικά ψηφία κατανέμονται μερικώς διακριτά, με κάποια clusters να είναι εμφανή. Ωστόσο, υπάρχει ακόμα κάποια αλληλοεπικάλυψη μεταξύ των σημείων για ορισμένα ψηφία, υποδεικνύοντας ότι η μείωση σε δύο διαστάσεις δεν είναι αρκετή για πλήρη διαχωρισμό των ψηφίων.

    \item Στο τρισδιάστατο διάγραμμα (Εικόνα~\ref{fig:pca_3d_plot}), η επιπλέον διάσταση προσθέτει λίγο περισσότερη διακριτότητα, αλλά εξακολουθούν να υπάρχουν επικαλύψεις. Η τρίτη διάσταση αυξάνει το ποσοστό της διασποράς που εξηγείται, ωστόσο η οπτική διάκριση παραμένει μερική.

\end{enumerate}

Η μείωση των διαστάσεων με PCA ήταν μερικώς επιτυχής. Αν και διατήρησε ένα σημαντικό ποσοστό της αρχικής διασποράς, οι πρώτες δύο και τρεις κύριες συνιστώσες δεν αρκούν για τον πλήρη διαχωρισμό των ψηφίων, λόγω της πολυπλοκότητας και της αλληλεξάρτησης των χαρακτηριστικών στα δεδομένα. Η προσθήκη επιπλέον συνιστωσών ενδέχεται να βοηθήσει, αλλά θα οδηγήσει σε μεγαλύτερη πολυπλοκότητα στην οπτικοποίηση. Παρ' όλα αυτά, η χρήση του PCA αποτελεί μια αποτελεσματική τεχνική για τη μείωση της διάστασης και την αρχική κατανόηση της κατανομής των χαρακτηριστικών.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/pca_2d_plot.png}
    \caption{PCA Scatter Plot σε δύο διαστάσεις για τα χαρακτηριστικά των ψηφίων.}
    \label{fig:pca_2d_plot}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{images/pca_3d_plot.png}
    \caption{PCA Scatter Plot σε τρεις διαστάσεις για τα χαρακτηριστικά των ψηφίων.}
    \label{fig:pca_3d_plot}
\end{figure}

\section*{Βήμα 7}

Για την ταξινόμηση των ηχητικών δεδομένων, ακολουθήθηκε η εξής διαδικασία:

Αρχικά, τα δεδομένα χωρίστηκαν σε σύνολα εκπαίδευσης (70\%) και ελέγχου (30\%)
χρησιμοποιώντας τη συνάρτηση \texttt{train\_test\_split} με \texttt{stratify=labels}
για να διασφαλιστεί η ισορροπημένη κατανομή των κλάσεων. Πριν την εκπαίδευση,
εφαρμόστηκε κανονικοποίηση στα δεδομένα χρησιμοποιώντας το \texttt{StandardScaler}.

Χρησιμοποιήθηκαν συνολικά 5 ταξινομητές:
\begin{enumerate}
    \item Custom Bayesian (από την πρώτη εργαστηριακή άσκηση)
    \item Gaussian Naive Bayes (από scikit-learn)
    \item SVM (με γραμμικό kernel)
    \item Random Forest (με 100 δέντρα)
    \item CatBoost
\end{enumerate}

Τα αρχικά αποτελέσματα έδειξαν παρόμοια απόδοση για όλους τους ταξινομητές:

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \hline
        \textbf{Ταξινομητής} & \textbf{Ακρίβεια} & \textbf{F1-Score} \\
        \hline
        Custom Bayesian      & 65.00\%           & 63.51\%           \\
        Gaussian Naive Bayes & 65.00\%           & 63.51\%           \\
        SVM                  & 65.00\%           & 62.52\%           \\
        Random Forest        & 75.00\%           & 74.88\%           \\
        CatBoost             & 67.50\%           & 65.30\%           \\
        \hline
    \end{tabular}
    \caption{Αρχικά αποτελέσματα ταξινόμησης}
\end{table}

\textbf{Bonus: Προσθήκη Επιπλέον Χαρακτηριστικών}

Για το bonus μέρος, προστέθηκαν επιπλέον χαρακτηριστικά στο διάνυσμα:

\begin{enumerate}
    \item Zero-Crossing Rate (ZCR): Αποτελεί ένα από τα βασικότερα χαρακτηριστικά στο
          πεδίο του χρόνου και υπολογίζει το ρυθμό με τον οποίο το σήμα αλλάζει πρόσημο
          \cite{kedem1986spectral}. Είναι ιδιαίτερα χρήσιμο για την αναγνώριση ομιλίας
          καθώς συσχετίζεται με το φασματικό περιεχόμενο του σήματος
          \cite{bachu2010voiced}.

    \item Πολυωνυμικά χαρακτηριστικά (Polynomial Features): Χρησιμοποιούνται για την καταγραφή της
          φασματικής παραμόρφωσης και την εκτίμηση της φασματικής περιβάλλουσας \cite{noll1967cepstrum}. Παρέχουν
          πληροφορίες για τη δομή των αρμονικών συχνοτήτων του σήματος.
\end{enumerate}

Η προσθήκη των επιπλέον χαρακτηριστικών οδήγησε σε σημαντική βελτίωση των αποτελεσμάτων:

\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
        \hline
        \textbf{Ταξινομητής} & \textbf{Ακρίβεια} & \textbf{F1-Score} & \textbf{Βελτίωση Ακρίβειας} \\
        \hline
        Custom Bayesian      & 65.00\%           & 63.84\%           & 0\%                         \\
        Gaussian Naive Bayes & 65.00\%           & 63.84\%           & 0\%                         \\
        SVM                  & 72.50\%           & 72.16\%           & +7.5\%                      \\
        Random Forest        & 65.00\%           & 65.05\%           & -2.5\%                      \\
        CatBoost             & 80.00\%           & 79.99\%           & +12.5\%                     \\
        \hline
    \end{tabular}
    \caption{Αποτελέσματα μετά την προσθήκη επιπλέον χαρακτηριστικών}
\end{table}

\textbf{Συμπεράσματα}

Η σημαντική βελτίωση μπορεί να αποδοθεί στο ότι τα επιπρόσθετα χαρακτηριστικά παρείχαν
συμπληρωματική πληροφορία για τη διάκριση των ψηφίων, ιδιαίτερα το ZCR που καταγράφει
τις μεταβολές του σήματος στο πεδίο του χρόνου και τα πολυωνυμικά χαρακτηριστικά που
αποτυπώνουν επιπλέον πληροφορία στο πεδίο της συχνότητας. Ο CatBoost φαίνεται να
αξιοποίησε καλύτερα αυτή την επιπρόσθετη πληροφορία, οδηγώντας στη σημαντική βελτίωση
της απόδοσής του τόσο στην ακρίβεια όσο και στο F1-Score (79.94\%). Αξιοσημείωτη
είναι επίσης η βελτίωση του SVM, που πέτυχε F1-Score 72.16\%. Αντίθετα, οι Bayesian
ταξινομητές και ο Random Forest δεν κατάφεραν να αξιοποιήσουν αποτελεσματικά τα
επιπλέον χαρακτηριστικά.

\section*{Βήμα 8}

\subsection*{Παραγωγή Δεδομένων}

Για την υλοποίηση του συστήματος πρόβλεψης, αρχικά δημιουργήθηκαν ακολουθίες 10 σημείων ημιτόνου και συνημιτόνου με συχνότητα $f = 40$ Hz. Η υλοποίηση πραγματοποιήθηκε μέσω της συνάρτησης \texttt{sample\_waves}:

\begin{itemize}
    \item Συχνότητα: $f = 40$ Hz
    \item Αριθμός σημείων: $n\_points = 10$
    \item Βήμα δειγματοληψίας: $step = f \cdot n\_points$
    \item Περίοδος: $T = \frac{1}{f}$
    \item $t_{\text{start}} \sim U(0,T)$
\end{itemize}

Τα σημεία υπολογίζονται ως:
\begin{equation}
    t_i = t_{start} + \frac{i}{step}, \quad i = 0,\ldots,n\_points-1
\end{equation}
\begin{equation}
    x_i = \sin(2\pi f t_i), \quad y_i = \cos(2\pi f t_i)
\end{equation}

\subsection*{Αρχιτεκτονική Νευρωνικών Δικτύων}

Υλοποιήθηκαν και συγκρίθηκαν τρεις διαφορετικές αρχιτεκτονικές αναδρομικών νευρωνικών δικτύων:

\begin{enumerate}
    \item \textbf{RNN}
    \item \textbf{LSTM (Long Short-Term Memory)}
    \item \textbf{GRU (Gated Recurrent Unit)}
\end{enumerate}

Η επιλογή των LSTM και GRU έναντι των απλών RNN βασίζεται σε σημαντικά πλεονεκτήματα που έχουν τεκμηριωθεί στη βιβλιογραφία:

\begin{itemize}
    \item \textbf{Αντιμετώπιση Vanishing Gradient}: Τα απλά RNNs υποφέρουν από το πρόβλημα του vanishing gradient \cite{bengio1994learning}, το οποίο καθιστά δύσκολη την εκπαίδευση σε μακροχρόνιες ακολουθίες. Τα LSTM και GRU, μέσω των μηχανισμών πύλης, επιτρέπουν καλύτερη ροή των gradients κατά την οπισθοδιάδοση \cite{pascanu2013difficulty}.

    \item \textbf{Μακροπρόθεσμη Μνήμη}: Η αρχιτεκτονική των LSTM σχεδιάστηκε ειδικά για να διατηρεί πληροφορίες για μεγάλα χρονικά διαστήματα \cite{gers1999learning}. Τα GRU προσφέρουν παρόμοια απόδοση με απλούστερη δομή \cite{chung2014empirical}.

    \item \textbf{Επιλεκτική Ενημέρωση}: Οι πύλες (gates) επιτρέπουν στο δίκτυο να μαθαίνει ποιες πληροφορίες είναι σημαντικές για διατήρηση και ποιες μπορούν να αγνοηθούν \cite{greff2016lstm}. Στα LSTM, αυτό επιτυγχάνεται μέσω τριών πυλών (input, forget, output), ενώ τα GRU χρησιμοποιούν δύο πύλες (reset, update) \cite{cho2014properties}.

    \item \textbf{Ευρεία Εφαρμογή}: Έχουν αποδειχθεί ιδιαίτερα αποτελεσματικά σε πολλές εφαρμογές όπως:
          \begin{itemize}
              \item Επεξεργασία φυσικής γλώσσας \cite{sutskever2014sequence}
              \item Πρόβλεψη χρονοσειρών \cite{zhang2018deep}
              \item Αναγνώριση ομιλίας \cite{graves2013speech}
          \end{itemize}
\end{itemize}

\subsection*{Εκπαίδευση και Αξιολόγηση}

Η εκπαίδευση πραγματοποιήθηκε με τις εξής παραμέτρους:
\begin{itemize}
    \item Συνάρτηση κόστους: Mean Squared Error (MSE)
    \item Βελτιστοποιητής: Adam με αρχικό learning rate 0.001
    \item Learning rate schedule με μείωση στα epochs 15, 30, και 40
    \item Batch size: 16
    \item Αριθμός epochs: 50
\end{itemize}

Τα δεδομένα χωρίστηκαν σε:
\begin{itemize}
    \item 70\% training set
    \item 20\% validation set
    \item 10\% test set
\end{itemize}

subsection*{Αποτελέσματα και Ανάλυση}

% edw pinakas me ta apotelesmata gia ta 3 models

\subsubsection*{Καμπύλες Εκπαίδευσης}

Από τις γραφικές παραστάσεις του σφάλματος εκπαίδευσης (training loss) και επικύρωσης (validation loss) για κάθε μοντέλο παρατηρούμε:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{gru_loss.png}
    \caption{Καμπύλες εκπαίδευσης GRU}
    \label{fig:gru_history}
\end{figure}

\begin{enumerate}
    \item \textbf{GRU}:
          \begin{itemize}
              \item Ταχεία σύγκλιση μέσα στα πρώτα 5 epochs
              \item Σταθεροποίηση του σφάλματος σε πολύ χαμηλές τιμές ($\approx 0.001$)
              \item Ελάχιστη διαφορά μεταξύ training και validation loss
          \end{itemize}

          \begin{figure}[h]
              \centering
              \includegraphics[width=0.8\textwidth]{lstm_loss.png}
              \caption{Καμπύλες εκπαίδευσης LSTM}
              \label{fig:lstm_history}
          \end{figure}

    \item \textbf{LSTM}:
          \begin{itemize}
              \item Παρόμοια συμπεριφορά με το GRU
              \item Ελαφρώς υψηλότερο αρχικό σφάλμα ($\approx 0.28$)
              \item Εξίσου καλή σύγκλιση σε χαμηλές τιμές σφάλματος
          \end{itemize}

          \begin{figure}[h]
              \centering
              \includegraphics[width=0.8\textwidth]{simple_rnn_loss.png}
              \caption{Καμπύλες εκπαίδευσης απλού RNN}
              \label{fig:rnn_history}
          \end{figure}

    \item \textbf{Απλό RNN}:
          \begin{itemize}
              \item Πιο αργή σύγκλιση συγκριτικά με LSTM και GRU
              \item Υψηλότερο τελικό σφάλμα ($\approx 0.06$)
              \item Εμφανής διαφορά μεταξύ training και validation loss
          \end{itemize}
\end{enumerate}

\subsubsection*{Ανάλυση Προβλέψεων}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{pred_step8.png}
    \caption{Σύγκριση προβλέψεων των μοντέλων με τις πραγματικές τιμές}
    \label{fig:predictions}
\end{figure}

Από τη σύγκριση των προβλέψεων με τις πραγματικές τιμές παρατηρούμε:

\begin{itemize}
    \item Τα LSTM και GRU παρουσιάζουν σχεδόν ταυτόσημη απόδοση με εξαιρετική ακρίβεια πρόβλεψης
    \item Το απλό RNN παρουσιάζει σημαντικές αποκλίσεις, ιδιαίτερα στα πρώτα σημεία της ακολουθίας
    \item Όλα τα μοντέλα συγκλίνουν καλύτερα στα μεσαία και τελικά σημεία της ακολουθίας
    \item Η μέγιστη απόκλιση του απλού RNN παρατηρείται στο πρώτο σημείο (0.05 έναντι -0.98)
\end{itemize}

Τα αποτελέσματα επιβεβαιώνουν την ανωτερότητα των αρχιτεκτονικών LSTM και GRU έναντι του απλού RNN για τη συγκεκριμένη εφαρμογή πρόβλεψης χρονοσειρών.

\section*{Αναγνώριση Ψηφίων με GMM-HMM}

\subsection*{Βήμα 9: Διαχωρισμός Δεδομένων}

Αρχικά, τα δεδομένα εκπαίδευσης διαχωρίστηκαν σε training και validation set με αναλογία 80\%-20\% χρησιμοποιώντας τη συνάρτηση \texttt{train\_test\_split} της βιβλιοθήκης scikit-learn. Η παράμετρος \texttt{stratify=labels} εξασφάλισε την ισορροπημένη κατανομή των ψηφίων στα δύο σύνολα:

\begin{verbatim}
X_train, X_val, y_train, y_val, spk_train, spk_val = train_test_split(
   X, y, spk, test_size=0.2, random_state=42, stratify=y)
\end{verbatim}

\subsection*{Βήμα 10: Υλοποίηση GMM-HMM}

\subsubsection*{Αρχικοποίηση Μοντέλου}
Υλοποιήθηκε ένα GMM-HMM μοντέλο για κάθε ψηφίο, με δυνατότητα παραμετροποίησης των εξής χαρακτηριστικών:
\begin{itemize}
    \item \texttt{n\_states}: Αριθμός καταστάσεων HMM (1-4)
    \item \texttt{n\_mixtures}: Αριθμός Gaussian κατανομών στο μείγμα (1-5)
    \item Δυνατότητα επιλογής μεταξύ απλής Gaussian και GMM κατανομής
\end{itemize}

\subsubsection*{Δομή Left-Right HMM}
Ο πίνακας μεταβάσεων $A = \{a_{ij}\}$ αρχικοποιήθηκε ως left-right με τους εξής περιορισμούς:
\begin{itemize}
    \item Για μία κατάσταση:
          \begin{itemize}
              \item $a_{00} = 1$ (πλήρης αυτομετάβαση)
          \end{itemize}
    \item Για πολλαπλές καταστάσεις:
          \begin{itemize}
              \item $a_{ij} = 0$ για $j < i$ (απαγόρευση μεταβάσεων προς τα πίσω)
              \item $a_{ij} = 0$ για $j > i + 1$ (επιτρέπονται μόνο μεταβάσεις σε διαδοχικές καταστάσεις)
              \item $a_{ii} = 0.5$ για $i < n-1$ (πιθανότητα παραμονής στην ίδια κατάσταση)
              \item $a_{i,i+1} = 0.5$ για $i < n-1$ (πιθανότητα μετάβασης στην επόμενη κατάσταση)
              \item $a_{n-1,n-1} = 1$ (η τελευταία κατάσταση είναι απορροφητική)
          \end{itemize}
    \item Αρχικές πιθανότητες: $\pi_i = \begin{cases} 1 & \text{για } i = 1 \\ 0 & \text{για } i \neq 1 \end{cases}$
    \item Τελικές πιθανότητες: $f_i = \begin{cases} 1 & \text{για κατάσταση } i = n \text{ ή } n=1 \\ 0 & \text{αλλιώς} \end{cases}$
\end{itemize}

\subsection*{Εκπαίδευση με τον Αλγόριθμο EM (Baum-Welch)}
Η εκπαίδευση των μοντέλων πραγματοποιήθηκε με τον αλγόριθμο Baum-Welch μέσω της βιβλιοθήκης Pomegranate:
\begin{verbatim}
model = DenseHMM(
   distributions=emission_model,
   edges=A,
   starts=start_probs,
   ends=end_probs,
   verbose=True,
   max_iter=100,
   tol=1e-3
).fit(data)
\end{verbatim}

Για κάθε κατάσταση, οι κατανομές εκπομπής προσαρμόζονται στα δεδομένα με δύο τρόπους:
\begin{itemize}
    \item \textbf{GMM}: Μείγμα Gaussian κατανομών με n\_mixtures συνιστώσες
    \item \textbf{Απλή Gaussian}: Μία Gaussian κατανομή ανά κατάσταση
\end{itemize}

Ο αλγόριθμος εκτελείται μέχρι:
\begin{itemize}
    \item Να φτάσει τις 100 επαναλήψεις (\texttt{max\_iter=100})
    \item Ή η μεταβολή της πιθανοφάνειας να είναι μικρότερη από $10^{-3}$ (\texttt{tol=1e-3})
\end{itemize}

\subsection*{Αξιολόγηση Μοντέλου}
Για την αξιολόγηση του μοντέλου:
\begin{itemize}
    \item Υπολογίζεται ο λογάριθμος της πιθανοφάνειας (\texttt{log\_probability}) για κάθε ακολουθία και κάθε μοντέλο
    \item Επιλέγεται το μοντέλο (ψηφίο) με τη μέγιστη πιθανοφάνεια ως πρόβλεψη
    \item Υπολογίζεται το σκορ ακρίβειας στο validation και test set
    \item Δημιουργούνται κανονικοποιημένα Confusion Matrix για την οπτικοποίηση των αποτελεσμάτων
\end{itemize}

% Insert image
\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{hmm_val_cm.png}
    \caption{Κανονικοποιημένο Validation Confusion Matrix για το μοντέλο GMM-HMM}
    \label{fig:hmm_confusion_matrix}
\end{figure}

\begin{figure}{h}
    \centering
    \includegraphics[width=0.8\textwidth]{hmm_test_cm.png}
    \caption{Κανονικοποιημένο Test Confusion Matrix για το μοντέλο GMM-HMM}
    \setlength{\emergencystretch}{1em}
    \label{fig:hmm_confusion_matrix}
\end{figure}

\subsection*{Εκπαίδευση Μοντέλων (Βήμα 11)}
Για την αναγνώριση ψηφίων, εκπαιδεύτηκαν 10 ξεχωριστά GMM-HMM μοντέλα (ένα για κάθε ψηφίο) με τον αλγόριθμο Expectation-Maximization. Διερευνήθηκε ένα εύρος παραμέτρων με:
\begin{itemize}
    \item Αριθμό καταστάσεων HMM: 1 έως 4
    \item Αριθμό Γκαουσιανών κατανομών: 1 έως 5
\end{itemize}

Η εκπαίδευση πραγματοποιήθηκε με τα εξής χαρακτηριστικά:
\begin{itemize}
    \item Μέγιστος αριθμός επαναλήψεων: 100
    \item Κριτήριο σύγκλισης: μεταβολή του λογαρίθμου πιθανοφάνειας < $10^{-3}$
    \item Χρήση όλων των διαθέσιμων δεδομένων για κάθε ψηφίο
\end{itemize}

\subsection*{Διαδικασία Αναγνώρισης και Αξιολόγησης (Βήμα 12)}
Μετά την εκπαίδευση, η αξιολόγηση πραγματοποιήθηκε σε δύο στάδια:
\begin{enumerate}
    \item Αρχική αξιολόγηση στο validation set για την εύρεση βέλτιστων παραμέτρων
    \item Τελική αξιολόγηση στο test set με τις βέλτιστες παραμέτρους
\end{enumerate}

Η διαδικασία αναγνώρισης για κάθε εκφώνηση περιελάμβανε:
\begin{itemize}
    \item Υπολογισμό του λογαρίθμου πιθανοφάνειας για κάθε μοντέλο
    \item Επιλογή του μοντέλου με τη μέγιστη πιθανοφάνεια ως τελική πρόβλεψη
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{hmm_test_acc_heatmap.png}
    \caption{Χάρτης θερμότητας ακρίβειας στο test set ως προς τον αριθμό καταστάσεων και μειγμάτων.
        Οι τιμές αντιπροσωπεύουν το ποσοστό ακρίβειας (accuracy) για κάθε συνδυασμό παραμέτρων.
        Σκοτεινότερο κόκκινο υποδεικνύει υψηλότερη ακρίβεια, ενώ το μπλε χαμηλότερη.}
    \label{fig:test_acc_heatmap}
\end{figure}

\subsection*{Ανάλυση Αποτελεσμάτων (Βήμα 13)}
Όπως φαίνεται στο Σχήμα \ref{fig:test_acc_heatmap}, τα αποτελέσματα της αξιολόγησης στο test set έδειξαν:

\subsubsection*{Επίδραση Παραμέτρων}
\begin{itemize}
    \item \textbf{Βέλτιστη επίδοση}: 99.7\% ακρίβεια με 2 καταστάσεις και 5 μείγματα Γκαουσιανών
    \item \textbf{Αριθμός καταστάσεων}:
          \begin{itemize}
              \item 2-3 καταστάσεις παρουσίασαν σταθερά υψηλή απόδοση
              \item 4 καταστάσεις έδειξαν μεγαλύτερη διακύμανση στην απόδοση
          \end{itemize}
    \item \textbf{Αριθμός μειγμάτων}:
          \begin{itemize}
              \item Αύξηση του αριθμού μειγμάτων γενικά βελτίωσε την απόδοση
              \item 3-5 μείγματα έδωσαν σταθερά καλά αποτελέσματα
          \end{itemize}
\end{itemize}

\subsubsection*{Σημαντικά Ευρήματα}
\begin{itemize}
    \item Υψηλή απόδοση (>95\%) για τις περισσότερες σταθερές διαμορφώσεις
    \item Ορισμένοι συνδυασμοί παραμέτρων (π.χ., 2-3 καταστάσεις με 1 μείγμα) έδειξαν πολύ χαμηλή απόδοση (10\%)
    \item Σταθερή υψηλή απόδοση στην περιοχή 3+ μειγμάτων για όλες τις καταστάσεις
\end{itemize}

Η διαδικασία αξιολόγησης σε δύο στάδια (validation και test) αποδείχθηκε κρίσιμη για:
\begin{itemize}
    \item Αποφυγή υπερπροσαρμογής (overfitting)
    \item Εύρεση βέλτιστων παραμέτρων χωρίς μεροληψία
    \item Αξιόπιστη εκτίμηση της πραγματικής απόδοσης του μοντέλου
\end{itemize}

Τα αποτελέσματα επιβεβαιώνουν την αποτελεσματικότητα της προσέγγισης GMM-HMM για την αναγνώριση μεμονωμένων ψηφίων,
με το μοντέλο να επιτυγχάνει εξαιρετική ακρίβεια στις βέλτιστες διαμορφώσεις του.

\section*{Βήμα 14}
\subsection*{1}

Σε αυτό το ερώτημα τρποποιήθηκαν τμήματα του κώδικα του αρχείου \texttt{lstm.py}
για τις ανάγκες του ερωτήματος 14. Συγκεκριμένα τροποιήθηκαν οι εξής κλάσσεις:

\begin{itemize}
    \item \textbf{EarlyStopping}
    \item \textbf{FrameLevelDataset}
    \item \textbf{BasicLSTM}
\end{itemize}

To \textbf{EarlyStopping} έχει υλοποιηθεί με κριτηρίο τερματισμού το validation
loss, δηλαδή αν δεν βελτιώνεται (μικραίνει το σφάλμα) να τερματίζει η
διαδικασία του training. H υλοποίηση του \textbf{FrameLevelDataset} έχει ως
στόχο την δημιουργία ενός dataset το οποίο θα επιστρέφει ακολουθίες με μήκος
ίσο με το μέγιστο μήκος από τις ακολουθίες που δόθηκαν ως όρισμα κατά την
αρχικοποιήση του FrameLevelDataset. Αυτό επιτυγχάνεται πραγματοποιώντας padding
στις ακολουθίες με μικρότερο μήκος από το μέγιστο μήκος. Τέλος, η κλάση
\textbf{BasicLSTM} είναι μια συνηθισμένη υλοποίηση ενός LSTM δικτύου στο οποίο
έχει προστεθεί ένα dropout layer για την μείωση του overfitting και ένα fc
layer για τον μετασχηματισμό των εξόδων του LSTM σε κατάλληλο shape για την
πραγματοποιήση του classification. Επιπλέον, από τα αποτέλεσμα που επιστρέφει
το lstm layer, δηλαδή όλα τα $h_t$, εξάγεται το $h_t$, το οποίο αντιστοιχεί στο
τελευταίο χρονικό βήμα της ακολουθίας, δηλαδή πριν το padding. Αυτο
επιτυχαγχάνεται με την χρήση της μεθόδου \texttt{last\_timestep}

Επιπρόσθετα τροποποιήθηκαν οι παρακάτω συναρτήσεις, οι οποίες σχετίζονται
με την εκπαίδευση και την αξιολόγηση του μοντέλου:

\begin{itemize}
    \item \texttt{create\_dataloaders}
    \item \texttt{training\_loop}
    \item \texttt{evaluation\_loop}
    \item \texttt{train}
\end{itemize}

Η συνάρτηση \texttt{create\_dataloaders} δημιουργεί τα dataloaders για το training το validation και test set.
Η συνάρτηση \texttt{training\_loop} είναι υπεύθυνη για την εκπαίδευση του μοντέλου. Είναι η συνάρτηση στην οποία
πραγματοποιούνται οι διαδικασίες για την κάθε εποχής, όπως:

\begin{itemize}
    \item Cast των μεταβλητών στο κατάλληλο device
    \item Μηδενισμός των gradient Values
    \item Υπολογισμός του loss
    \item Πραγματοποιήση backpropagation
    \item Ενημέρωση των βαρών
\end{itemize}

Παρόμοια, η συνάρτηση \texttt{evaluation\_loop} είναι υπεύθυνη
για την αξιολόγηση του μοντέλου στο validation set, χωρίς να γίνεται backpropagation.
Τέλος, η συνάρτηση \texttt{train} είναι υπεύθυνη για την διαδικασίας της εκπαίδευση του μοντέλου.
Μέσα σε αυτή την συνάρτηση καλούνται οι παραπάνω συναρτήσεις, αρχικοποιείται ο optimizer και το
EarlyStopping με τα κατάλληλα κριτήρια τερματισμού, μαζί με την αποθήκευση των βέλτιστων βαρών
του μοντέλου. Ως έξοδος της συνάρτησης επιστρέφεται το μοντέλο με τα καλύτερα βάρη, το συνολικό χρόνο
εκπαίδευσης και τα σφάλματα του μοντέλου στο training και validation set.

\subsection*{2}

Βλ. το αρχείο \texttt{lstm.py} για την υλοποίηση του LSTM δικτύου.

%% ή ενα σνιππετ απο τον κώδικα με το ινιτ και το φοργορντ

Παράμετροι που χρησιμοποιήθηκαν για την αρχικοποιήση του LSTM:

\begin{itemize}
    \item \textbf{Hidden size}: 64
    \item \textbf{Number of layers}: 2
    \item \textbf{Output size}: 10
    \item \textbf{Epochs}: 12
    \item \textbf{Learning rate}: 0.001
    \item \textbf{Bidirectional}: False
    \item \textbf{Batch size}: 32
\end{itemize}

\subsection*{3}

Εκπαιδεύοντας το μοντέλο με optimizer τον αλγόριθμο Adam, το learning rate 0.001 και το loss function το \texttt{CrossEntropyLoss},
όπου το συγκεκριμένο loss function είναι κατάλληλο για το πρόβλημα της mutliclass ταξινόμησης,  το μοντέλο εκπαιδεύτηκε για 12 εποχές.

Η πορεία του training loss και του validation loss φαίνεται στο Σχήμα \ref{fig:loss_plot_lstm}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{tr_loss_lstm.png}
    \caption{Πορεία του training loss κατά την διάρκεια της εκπαίδευσης}
    \label{fig:loss_plot_lstm}
\end{figure}

Αρχικά, το μοντέλο παρουσιάζει υψηλή τιμή απώλειας $L_0 \approx 2.0$ στην εποχή $e_0$, που είναι χαρακτηριστικό της φάσης αρχικοποίησης της εκπαίδευσης.
Παρατηρείται σημαντική μείωση της απώλειας κατά τη διάρκεια των εποχών $e_0$ έως $e_2$, που αναπαρίσταται από τη συνάρτηση:

\[
    \frac{\Delta L}{\Delta e} \gg 0 \text{ για } e \in [0,2]
\]

Ο ρυθμός βελτίωσης μειώνεται μετά το $e_2$, με τη συνάρτηση απώλειας $L(e)$ να επιδεικνύει ασυμπτωτική συμπεριφορά:

\[
    \lim_{e \to 10} L(e) \approx 0.1
\]

Η καμπύλη απώλειας παρουσιάζει χαρακτηριστικά πλατό στο διάστημα $e \in [4,6]$, ακολουθούμενη από σταδιακή βελτιστοποίηση όπου:

\[
    \frac{d L}{d e} < 0 \text{ αλλά } \left|\frac{d L}{d e}\right| \ll \left|\frac{d L}{d e}\right|_{e \in [0,2]}
\]

Η συνολική δυναμική της εκπαίδευσης μπορεί να χαρακτηριστεί από μια μονότονα φθίνουσα συνάρτηση χωρίς σημαντικά τοπικά μέγιστα, υποδεικνύοντας βέλτιστες ιδιότητες σύγκλισης:

\[
    L(e_1) > L(e_2) \text{ για όλα τα } e_1 < e_2 \text{ όπου } e \in [0,10]
\]

Η συμπεριφορά αυτή υποδηλώνει μια υγιή διαδικασία εκπαίδευσης, με την απουσία υπερπροσαρμογής και την επίτευξη σταθερής σύγκλισης προς ένα τοπικό ελάχιστο.

\subsection*{4}

Ξαναεκπαιδεύοντας το μοντέλο μας με τον ίδιο τρόπο, αλλά αυτή τη φορά εμφανίζοντας και το validation loss, λαμβάνουμε το Σχήμα \ref{fig:tr_val_loss_plot_lstm}.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{tr_val_loss_lstm.png}
    \caption{Πορεία του training και validation loss κατά την διάρκεια της εκπαίδευσης}
    \label{fig:tr_val_loss_plot_lstm}
\end{figure}

Το γράφημα απεικονίζει το training loss (μπλε) και validaiton loss (κόκκινο) για $12$ εποχές ενός μοντέλου LSTM.
Και οι δύο καμπύλες ξεκινούν από διαφορετικά σημεία (training στο $\sim2.0$, validation στο $\sim1.35$)
και δείχνουν ταχεία αρχική μείωση μέχρι την εποχή $2$. Γύρω στις εποχές $2$-$3$, οι καμπύλες τέμνονται και
αρχίζουν να ακολουθούν παρόμοια πορεία, συνεχίζοντας να μειώνονται αλλά με πολύ βραδύτερο ρυθμό. Μετά την εποχή $6$,
οι καμπύλες παραμένουν πολύ κοντά αλλά δείχνουν ελαφρά απόκλιση, με το training loss να πέφτει ελαφρώς κάτω από το validation loss.

Το validation loss παρουσιάζει μικρές διακυμάνσεις στις τελευταίες εποχές ($8$-$12$), ενώ το training loss διατηρεί
μια πιο ομαλή πτωτική τάση. Σημαντικό είναι ότι το validation loss δεν δείχνει σημαντική αύξηση σε σύγκριση με το trainning loss,
υποδεικνύοντας ότι το μοντέλο δεν κάνει overfitting. Και τα 2 είδη loss συγκλίνουν σε τιμές
γύρω στο $0.1$-$0.2$ μέχρι την εποχή $11$, υποδεικνύοντας καλή απόδοση και γενίκευση του μοντέλου.

\subsection*{5}
Το dropout και το L2 Regularization είναι δύο τεχνικές που
χρησιμοποιούνται για να μειώσουν το overfitting στα νευρωνικά δίκτυα.
Το dropout είναι μια τεχνική που εφαρμόζεται στα layers του
νευρωνικού δικτύου κατά την εκπαίδευση, τα οποία ανάλογα με την ορίσμενη
πιθανότητα, κάνουν "drop" (μηδενίζουν) κάποια στοιχεία που δέχονται ως
είσοδο. Στην ουσία μηδενίζουν κάποια στοιχεία που θα δωθούν στο επόμενο
επίπεδο του δικτύου. Από την άλλη, το L2 Regularization είναι μια
τεχνική που προστίθεται στο loss function, το οποίο δίνει "ποίνη"
στα βάρη του μοντέλου όταν αυτά γίνονται πολύ μεγάλα.

\subsection*{6}

To \textbf{EarlyStopping} έχει 2 χρησιμότητες:

\begin{enumerate}
    \item Επίτρεπει την περάτωση του δικτού κατά την εκπαίδευση, ανάλογα το κριτηρίο τερματισμού
          που έχει ορισθεί. Αυτό έχει ως αποτέλεσμα την εξοικονόμηση χρόνου και πόρων,
          καθώς δεν θα εκπαιδευθεί το μοντέλο για περισσότερες εποχές από όσο χρειάζεται.

    \item Είναι μία τεχνική που χρησιμοποιείται για την αποφυγή του overfitting.
          Η "έγκαιρη" περάτωση του μοντέλου, ουσιαστικά δεν του επιτρέπει να μάθει
          επιπλέον patterns τα οποία μπορεί να χαρακτηρίζουν το training set.
\end{enumerate}

Στη συνέχεια, με την αξιοποίηση της κλάσης \textbf{EarlyStopping} μπορούμε να επιτύχουμε την αυτόματη
αποθήκευση του μοντέλου με τα βέλτιστα βάρη, όταν το validation loss δεν βελτιώνεται για έναν
συγκεκριμένο αριθμό εποχών (στην περίπτωσή μας για $3$ εποχές). Με αυτόν τον τρόπο, αποθηκεύουμε
το μοντέλο με τα βέλτιστα βάρη, το οποίο μπορεί να χρησιμοποιηθεί για την εκτίμηση της απόδοσης
του μοντέλου στο test set.

Συγκεκριμένα, χρησιμοποιούμε τη συνάρτηση texttt{torch.save} για την αποθήκευση του μοντέλου
σε μορφή \texttt{pickle}, το οποίο μπορεί να ανακτηθεί αργότερα με τη χρήση της συνάρτησης
\texttt{torch.load}. 


\subsection*{7}
Σε ένα κλασίκο μοντέλο RNN (LSTM) ως είσοδος δίνεται μία ακολουθία $x_t$ για $t=1,2...T$
η οποία επεξεργάζεται προς τα μία χρονική κατέυθνηση από το μοντέλο και παράγει για κάθε $t$
ένα διάνυσμα $h_t$. Στην περίπτωση του bidirectional LSTM, το μοντέλο επεξεργάζεται την ακολουθία
προς 2 κατευθύνσεις, δηλαδή από το πρώτο χρονικό βήμα προς το τελευταίο και αντίστροφα, δηλαδή
σαν είχε δοθεί ως είσοδος η ακολουθια $x_t$ για $t=Τ,Τ-1...1$.
Έτσι,για κάθε χρονικό βήμα $t$ το μοντέλο παράγει δύο διανύσματα $h_t^{(f)}$ και $h_t^{(b)}$ τα οποία
αντιστοιχούν στην κατεύθυνση προς τα μπροστά και προς τα πίσω αντίστοιχα. Τα δύο αυτά διανύσματα
συνδυάζονται συνήθως με concatenate για την παραγωγή του τελικού διανύσματος $h_t$. Συνεπώς το
$h_t$ θα έχει διπλάσιο μέγεθος από το αρχικό διάνυσμα $h_t$ που παράγει ένα κλασικό LSTM.
Αυτό έχει ως αποτέλεσμα το μοντέλο να είναι περισσότερο υπολογιστικά ακριβές αλλά έχει το πλεονέκτημα
πως σε κάθε χρονικό βήμα της ακολουθίας συνδυάζει πληροφορίες και από το παρελθόν και από το μέλλον,
το οποίο σε πολλές περιπτώσεις μπορεί να βελτιώσει την απόδοση του μοντέλου.

\subsection*{8}
Για την υλοποιήση του ερωτήματος αυτού έχει δημιουργηθεί η συνάρτηση
\texttt{collate\_fn} η οποία δίνεται σαν όρισμα στον dataloader.
Ο ρόλος της \texttt{collate\_fn} είναι όταν καλείται ο dataloader
να επιστρέφει batches τα οποία είναι ταξινομημένα κατά φθίνουσα σειρά
με βάση το μήκος των ακολουθιών και ύστερα να πραγματοποιεί padding
στις ακολουθίες ώστε να έχουν το ίδιο μήκος. Επίσης, τα labels και
τα lengths των ακολουθιών αλλάζουν και αυτά με βάση την ταξινόμηση.

Το επόμενος μέρος κώδικα που τροποποιήθηκε είναι η μέγεθος
\texttt{forward} της κλάσσης \textbf{BasicLSTM}. Σε αυτό το σημείο, αυτό που
προστέθηκε είναι η χρήση του \texttt{pack\_padded\_sequence} το οποίο
φέρνει τα inputs σε μια packed μορφή που μπορεί να
χρησιμοποιήσει το LSTM. Οι έξοδοι του LSTM είναι σε packed μορφή
και για να έρθουν στην αναμενόμενη μορφή τους μορφή
(N, seq\_leq, hidden\_size) χρησιμοποιείται το \texttt{pad\_packed\_sequence}.
Τέλος, καθώς υπάρχουν διαφορετικά μήκη ακολουθιών,
το τελευταίο χρονικό βήμα της ακολουθίας εξάγεται με την
χρήση της μεθόδου \texttt{last\_timestep}

Όπως ήταν αναμένομενο για μεγάλα \texttt{hidden\_size} και \texttt{num\_layers} το μοντέλο
εκπαιδεύτεται αρκέτα πιο γρήγορα σε σύγκριση με το μοντέλο που δέχεται
τα ορίσματα σε κανονική μορφή. Αυτό συμβαίνει γιατί το packed sequence
επιτρέπει στο LSTM να μην υπολογίζει τα μηδενικά στοιχεία του padding
και έτσι επιταχύνει την διαδικασία του training. Βέβαια, σε μικρά αρχικοποιήμενα
δίκτυα το packed sequence δεν έχει κάποιο ιδιαίτερο όφελος
στην ταχύτητα του training, αντίθετα μπορεί να υπάρξει κάποια αύξηση
στον χρόνο εκπαίδευσης, λόγω του sorting των ακολουθιών που
πραγματοποιείται.

Επιπλέον, το μοντέλο εμφανίζει καλύτερη απόδοση στο validation set σε σχέση με το μοντέλο
που δέχεται τα ορίσματα σε κανονική μορφή. Το τελικό training και validation loss graph
με τις τελικές παραμέτρους του μοντέλου μας είναι το παρακάτω (Σχήμα \ref{fig:tr_val_loss_plot_lstm_packed})
καθώς και το test confusion matrix (Σχήμα \ref{fig:confusion_matrix_lstm_packed}), με τελικό validation 
accuracy $0.9926$ και test accuracy $0.9967$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{tr_val_loss_lstm_packed.png}
    \caption{Πορεία του training και validation loss κατά την διάρκεια της εκπαίδευσης με packed sequence}
    \label{fig:tr_val_loss_plot_lstm_packed}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{confusion_matrix_lstm_packed.png}
    \caption{Confusion matrix του μοντέλου με packed sequence}
    \label{fig:confusion_matrix_lstm_packed}
\end{figure}

Τελικές παράμετροι:

\begin{itemize}
    \item \textbf{Hidden size}: 128
    \item \textbf{Number of layers}: 5
    \item \textbf{Dropout}: 0.6
    \item \textbf{Epochs}: 50
    \item \textbf{Learning rate}: 0.001
    \item \textbf{Bidirectional}: True
    \item \textbf{Batch size}: 64
    \item \textbf{EarlyStopping patience}: 3
\end{itemize}



\bibliographystyle{plainnat}
\bibliography{references}

\end{document}